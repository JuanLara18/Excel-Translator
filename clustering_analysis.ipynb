{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kneed import KneeLocator\n",
    "from collections import Counter\n",
    "import umap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(file_path):\n",
    "    \"\"\"Load and preprocess the data.\"\"\"\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_stata(file_path)\n",
    "    \n",
    "    # Define columns of interest\n",
    "    cols = [\"MeasureDescription_EN\", \"CauseDescription_EN\", \"TechnicalObjectDescription_EN\"]\n",
    "    df[cols] = df[cols].fillna(\"\")\n",
    "    \n",
    "    # Filter rows where all columns are empty\n",
    "    df[\"all_empty\"] = df[cols].apply(lambda row: all(cell.strip() == \"\" for cell in row), axis=1)\n",
    "    df_filtered = df[~df[\"all_empty\"]].copy()\n",
    "    \n",
    "    # Combine text fields\n",
    "    df_filtered[\"text_combined\"] = df_filtered.apply(\n",
    "        lambda row: \" \".join([row[col].strip() for col in cols if row[col].strip() != \"\"]), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    print(f\"Processed {len(df_filtered)} valid records out of {len(df)} total records\")\n",
    "    return df_filtered\n",
    "\n",
    "def vectorize_text(df, max_features=1000):\n",
    "    \"\"\"Vectorize text using TF-IDF.\"\"\"\n",
    "    print(\"Vectorizing text...\")\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),  # Include bigrams\n",
    "        min_df=5,  # Minimum document frequency\n",
    "        max_df=0.95  # Maximum document frequency\n",
    "    )\n",
    "    \n",
    "    X = vectorizer.fit_transform(df[\"text_combined\"])\n",
    "    print(f\"Created {X.shape[1]} features from text\")\n",
    "    return X, vectorizer\n",
    "\n",
    "def determine_optimal_clusters(X, max_k=20):\n",
    "    \"\"\"Determine optimal number of clusters using multiple metrics.\"\"\"\n",
    "    print(\"Determining optimal number of clusters...\")\n",
    "    \n",
    "    # Initialize metrics\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    ch_scores = []\n",
    "    \n",
    "    # Reduce dimensionality for silhouette analysis\n",
    "    svd = TruncatedSVD(n_components=100)\n",
    "    X_reduced = svd.fit_transform(X)\n",
    "    \n",
    "    # Calculate metrics for different k values\n",
    "    for k in range(2, max_k + 1):\n",
    "        print(f\"Testing k={k}...\", end='\\r')\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(X_reduced, labels))\n",
    "        ch_scores.append(calinski_harabasz_score(X_reduced, labels))\n",
    "    \n",
    "    # Find optimal k using elbow method\n",
    "    kn = KneeLocator(\n",
    "        range(2, max_k + 1), inertias, \n",
    "        curve='convex', direction='decreasing'\n",
    "    )\n",
    "    k_elbow = kn.elbow\n",
    "    \n",
    "    # Find optimal k using silhouette score\n",
    "    k_silhouette = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "    \n",
    "    # Find optimal k using Calinski-Harabasz score\n",
    "    k_ch = ch_scores.index(max(ch_scores)) + 2\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Elbow curve\n",
    "    axes[0].plot(range(2, max_k + 1), inertias)\n",
    "    axes[0].set_title('Elbow Method')\n",
    "    axes[0].set_xlabel('k')\n",
    "    axes[0].set_ylabel('Inertia')\n",
    "    axes[0].axvline(x=k_elbow, color='r', linestyle='--')\n",
    "    \n",
    "    # Silhouette score\n",
    "    axes[1].plot(range(2, max_k + 1), silhouette_scores)\n",
    "    axes[1].set_title('Silhouette Score')\n",
    "    axes[1].set_xlabel('k')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    axes[1].axvline(x=k_silhouette, color='r', linestyle='--')\n",
    "    \n",
    "    # Calinski-Harabasz score\n",
    "    axes[2].plot(range(2, max_k + 1), ch_scores)\n",
    "    axes[2].set_title('Calinski-Harabasz Score')\n",
    "    axes[2].set_xlabel('k')\n",
    "    axes[2].set_ylabel('Score')\n",
    "    axes[2].axvline(x=k_ch, color='r', linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nOptimal number of clusters:\")\n",
    "    print(f\"Elbow method: {k_elbow}\")\n",
    "    print(f\"Silhouette score: {k_silhouette}\")\n",
    "    print(f\"Calinski-Harabasz score: {k_ch}\")\n",
    "    \n",
    "    # Return consensus k (mode of the three methods)\n",
    "    optimal_k = Counter([k_elbow, k_silhouette, k_ch]).most_common(1)[0][0]\n",
    "    return optimal_k\n",
    "\n",
    "def perform_clustering(X, optimal_k, vectorizer):\n",
    "    \"\"\"Perform clustering with optimal k.\"\"\"\n",
    "    print(f\"\\nPerforming clustering with k={optimal_k}...\")\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Get top terms per cluster\n",
    "    def get_top_terms(cluster_center, n_terms=10):\n",
    "        terms = vectorizer.get_feature_names_out()\n",
    "        top_indices = cluster_center.argsort()[-n_terms:][::-1]\n",
    "        return [terms[i] for i in top_indices]\n",
    "    \n",
    "    print(\"\\nTop terms per cluster:\")\n",
    "    for i, center in enumerate(kmeans.cluster_centers_):\n",
    "        print(f\"\\nCluster {i}:\")\n",
    "        print(\", \".join(get_top_terms(center)))\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def visualize_clusters(X, labels):\n",
    "    \"\"\"Create UMAP visualization of clusters.\"\"\"\n",
    "    print(\"\\nCreating cluster visualization...\")\n",
    "    \n",
    "    # Reduce dimensionality for visualization\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    X_reduced = reducer.fit_transform(X.toarray())\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=labels, cmap='tab20')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('UMAP visualization of clusters')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# File path\n",
    "file_path = \"/export/projects1/rsadun_bmw/03 Workplace/Clean Data/Production/Translation/Maintenance_Full_Translated.dta\"\n",
    "    \n",
    "# Load and preprocess data\n",
    "df = load_and_preprocess(file_path)\n",
    "\n",
    "# Vectorize text\n",
    "X, vectorizer = vectorize_text(df)\n",
    "\n",
    "# Determine optimal number of clusters\n",
    "optimal_k = determine_optimal_clusters(X)\n",
    "\n",
    "# Perform clustering\n",
    "labels = perform_clustering(X, optimal_k, vectorizer)\n",
    "\n",
    "# Visualize results\n",
    "visualize_clusters(X, labels)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['cluster'] = labels\n",
    "\n",
    "# Save results\n",
    "output_path = \"/export/projects1/rsadun_bmw/03 Workplace/Clean Data/Production/Translation/Maintenance_Full_Translated_Clustered_Optimal.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"\\nResults saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
