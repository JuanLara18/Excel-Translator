{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add([\"DataFrames\", \"CSV\", \"StatsBase\", \"Clustering\", \"TextAnalysis\", \"Plots\", \"Distances\", \"UMAP\", \"TSVD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.9/Project.toml`\n",
      "  \u001b[90m[5789e2e9] \u001b[39m\u001b[92m+ FileIO v1.16.1\u001b[39m\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.add([\"StatFiles\", \"DataFrames\", \"FileIO\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Invalid UTF-8 string",
     "output_type": "error",
     "traceback": [
      "Invalid UTF-8 string\n",
      "\n",
      "Stacktrace:\n",
      "  [1] error(s::String)\n",
      "    @ Base ./error.jl:35\n",
      "  [2] utf8proc_error(result::Int64)\n",
      "    @ Base.Unicode ./strings/unicode.jl:146\n",
      "  [3] utf8proc_decompose\n",
      "    @ ./strings/unicode.jl:155 [inlined]\n",
      "  [4] utf8proc_map(str::String, options::Int64, chartransform::Function)\n",
      "    @ Base.Unicode ./strings/unicode.jl:167\n",
      "  [5] #normalize#1\n",
      "    @ ./strings/unicode.jl:225 [inlined]\n",
      "  [6] normalize\n",
      "    @ ./strings/unicode.jl:187 [inlined]\n",
      "  [7] #normalize#1\n",
      "    @ /usr/local/app/rcs_bin/grid3/envs/rcs_2023.09/opt/julia-1.9.0/share/julia/stdlib/v1.9/Unicode/src/Unicode.jl:118 [inlined]\n",
      "  [8] normalize\n",
      "    @ /usr/local/app/rcs_bin/grid3/envs/rcs_2023.09/opt/julia-1.9.0/share/julia/stdlib/v1.9/Unicode/src/Unicode.jl:118 [inlined]\n",
      "  [9] normalizename(name::String)\n",
      "    @ CSV ~/.julia/packages/CSV/OnldF/src/utils.jl:335\n",
      " [10] #10\n",
      "    @ ./none:0 [inlined]\n",
      " [11] iterate\n",
      "    @ ./generator.jl:47 [inlined]\n",
      " [12] collect(itr::Base.Generator{Vector{String}, CSV.var\"#10#13\"{Bool}})\n",
      "    @ Base ./array.jl:782\n",
      " [13] detectcolumnnames(buf::Vector{UInt8}, headerpos::Int64, datapos::Int64, len::Int64, options::Parsers.Options, header::Any, normalizenames::Bool, oq::UInt8, eq::UInt8, cq::UInt8, cmt::Nothing, ignoreemptyrows::Bool)\n",
      "    @ CSV ~/.julia/packages/CSV/OnldF/src/detection.jl:185\n",
      " [14] CSV.Context(source::CSV.Arg, header::CSV.Arg, normalizenames::CSV.Arg, datarow::CSV.Arg, skipto::CSV.Arg, footerskip::CSV.Arg, transpose::CSV.Arg, comment::CSV.Arg, ignoreemptyrows::CSV.Arg, ignoreemptylines::CSV.Arg, select::CSV.Arg, drop::CSV.Arg, limit::CSV.Arg, buffer_in_memory::CSV.Arg, threaded::CSV.Arg, ntasks::CSV.Arg, tasks::CSV.Arg, rows_to_check::CSV.Arg, lines_to_check::CSV.Arg, missingstrings::CSV.Arg, missingstring::CSV.Arg, delim::CSV.Arg, ignorerepeated::CSV.Arg, quoted::CSV.Arg, quotechar::CSV.Arg, openquotechar::CSV.Arg, closequotechar::CSV.Arg, escapechar::CSV.Arg, dateformat::CSV.Arg, dateformats::CSV.Arg, decimal::CSV.Arg, groupmark::CSV.Arg, truestrings::CSV.Arg, falsestrings::CSV.Arg, stripwhitespace::CSV.Arg, type::CSV.Arg, types::CSV.Arg, typemap::CSV.Arg, pool::CSV.Arg, downcast::CSV.Arg, lazystrings::CSV.Arg, stringtype::CSV.Arg, strict::CSV.Arg, silencewarnings::CSV.Arg, maxwarnings::CSV.Arg, debug::CSV.Arg, parsingdebug::CSV.Arg, validate::CSV.Arg, streaming::CSV.Arg)\n",
      "    @ CSV ~/.julia/packages/CSV/OnldF/src/context.jl:470\n",
      " [15] #File#32\n",
      "    @ ~/.julia/packages/CSV/OnldF/src/file.jl:222 [inlined]\n",
      " [16] top-level scope\n",
      "    @ ~/Desktop/File-Translator/clustering_analysis_julia.ipynb:114"
     ]
    }
   ],
   "source": [
    "using DataFrames, CSV, StatsBase, Clustering, TextAnalysis, Plots, Distances, UMAP, TSVD\n",
    "\n",
    "function elbow_method(inertias)\n",
    "    diffs = diff(inertias)\n",
    "    second_diffs = diff(diffs)\n",
    "    return argmax(second_diffs) + 1  # Index where the second derivative is highest\n",
    "end\n",
    "\n",
    "function determine_optimal_clusters(X, max_k=20)\n",
    "    println(\"Determining optimal number of clusters...\")\n",
    "\n",
    "    inertias = Float64[]\n",
    "    silhouette_scores = Float64[]\n",
    "    ch_scores = Float64[]\n",
    "\n",
    "    # Reduce dimensionality\n",
    "    svd = TSVD.tsvd(X, 100)\n",
    "    X_reduced = svd.U * Diagonal(svd.S)\n",
    "\n",
    "    for k in 2:max_k\n",
    "        println(\"Testing k=\", k)\n",
    "        kmeans_result = kmeans(X', k; maxiter=300, display=:none)\n",
    "        push!(inertias, sum(kmeans_result.costs))\n",
    "        push!(silhouette_scores, mean(silhouettes(X_reduced, kmeans_result.assignments, Euclidean())))\n",
    "        push!(ch_scores, calinski_harabasz_score(X_reduced, kmeans_result.assignments))\n",
    "    end\n",
    "\n",
    "    k_elbow = elbow_method(inertias) + 1  # Use elbow method replacement\n",
    "    k_silhouette = argmax(silhouette_scores) + 1\n",
    "    k_ch = argmax(ch_scores) + 1\n",
    "\n",
    "    println(\"\\nOptimal number of clusters:\")\n",
    "    println(\"Elbow method: \", k_elbow)\n",
    "    println(\"Silhouette score: \", k_silhouette)\n",
    "    println(\"Calinski-Harabasz score: \", k_ch)\n",
    "\n",
    "    return mode([k_elbow, k_silhouette, k_ch])\n",
    "end\n",
    "\n",
    "using DataFrames, FileIO, StatFiles\n",
    "\n",
    "function load_and_preprocess(file_path)\n",
    "    println(\"Loading and preprocessing data...\")\n",
    "\n",
    "    # Read Stata (.dta) file correctly\n",
    "    df = DataFrame(load(file_path))  # Use StatFiles for compatibility\n",
    "\n",
    "    # Define columns of interest\n",
    "    cols = [\"MeasureDescription_EN\", \"CauseDescription_EN\"]\n",
    "    for col in cols\n",
    "        df[!, col] .= coalesce.(df[!, col], \"\")  # Replace missing values with empty string\n",
    "    end\n",
    "\n",
    "    # Filter rows where all columns are empty\n",
    "    df[!, \"all_empty\"] = [all(strip(cell) == \"\" for cell in row) for row in eachrow(df[:, cols])]\n",
    "    df_filtered = filter(row -> !row.all_empty, df)\n",
    "\n",
    "    # Combine text fields\n",
    "    df_filtered[!, \"text_combined\"] = [join(filter(!isempty, strip.(row)), \" \") for row in eachrow(df_filtered[:, cols])]\n",
    "\n",
    "    println(\"Processed \", size(df_filtered, 1), \" valid records out of \", size(df, 1), \" total records\")\n",
    "    return df_filtered\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "# Vectorize text\n",
    "function vectorize_text(df, max_features=1000)\n",
    "    println(\"Vectorizing text...\")\n",
    "    \n",
    "    vectorizer = TFIDF()\n",
    "    corpus = StringDocument.(df.text_combined)\n",
    "    for doc in corpus\n",
    "        prepare!(doc, strip=true, lowercase=true, stopwords=true, stem=false)\n",
    "    end\n",
    "    \n",
    "    fit!(vectorizer, corpus)\n",
    "    X = transform(vectorizer, corpus)\n",
    "    \n",
    "    println(\"Created \", size(X, 2), \" features from text\")\n",
    "    return X, vectorizer\n",
    "end\n",
    "\n",
    "# Perform clustering\n",
    "function perform_clustering(X, optimal_k, vectorizer)\n",
    "    println(\"Performing clustering with k=\", optimal_k, \"...\")\n",
    "    kmeans_result = kmeans(X', optimal_k; maxiter=300, display=:none)\n",
    "    labels = kmeans_result.assignments\n",
    "    \n",
    "    println(\"\\nTop terms per cluster:\")\n",
    "    terms = vectorizer.vocab\n",
    "    for i in 1:optimal_k\n",
    "        top_indices = sortperm(kmeans_result.centers[i, :], rev=true)[1:10]\n",
    "        println(\"\\nCluster \", i, \":\")\n",
    "        println(join(terms[top_indices], \", \"))\n",
    "    end\n",
    "    \n",
    "    return labels\n",
    "end\n",
    "\n",
    "# Visualize clusters\n",
    "function visualize_clusters(X, labels)\n",
    "    println(\"\\nCreating cluster visualization...\")\n",
    "    reducer = UMAPReducer()\n",
    "    X_reduced = reduce(reducer, X)\n",
    "    \n",
    "    scatter(X_reduced[:, 1], X_reduced[:, 2], group=labels, xlabel=\"UMAP1\", ylabel=\"UMAP2\", legend=false, title=\"UMAP visualization of clusters\")\n",
    "end\n",
    "\n",
    "# Main execution\n",
    "file_path = \"/export/projects1/rsadun_bmw/03 Workplace/Clean Data/Production/Translation/Maintenance_Full_Step2.dta\"\n",
    "data = read(file_path, String)  # Read as raw text\n",
    "clean_data = transcode(String, data)  # Ensure UTF-8 encoding\n",
    "df = DataFrame(CSV.File(IOBuffer(clean_data), normalizenames=true))\n",
    "X, vectorizer = vectorize_text(df)\n",
    "optimal_k = determine_optimal_clusters(X)\n",
    "labels = perform_clustering(X, optimal_k, vectorizer)\n",
    "visualize_clusters(X, labels)\n",
    "\n",
    "df.cluster = labels\n",
    "output_path = \"/export/projects1/rsadun_bmw/03 Workplace/Clean Data/Production/Translation/Maintenance_Full_Translated_Clustered_Optimal.csv\"\n",
    "CSV.write(output_path, df)\n",
    "println(\"\\nResults saved to: \", output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
